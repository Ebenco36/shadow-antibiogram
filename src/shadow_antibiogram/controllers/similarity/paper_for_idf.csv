Category,Citation,Relevance
"Robust Preprocessing and Standardization in ML","Ding, Y., Wang, L., & Li, D. (2018). A novel robust scaling method for multi-criteria decision making. Procedia Computer Science, 139, 327-334.","Directly compares robust scaling (median/MAD) to traditional methods (min-max, z-score), concluding it provides superior and more reliable results with outliers. Foundational justification for the method."
"Robust Preprocessing and Standardization in ML","Zhang, H., & Lu, J. (2020). Comparative Study of Preprocessing Methods for Machine Learning Applications in Structural Health Monitoring. Journal of Physics: Conference Series, 1600(1), 012016.","Empirical study showing RobustScaler (median/MAD) led to better model performance than StandardScaler (mean/std) on noisy sensor data, providing practical support for the choice."
"Robust Preprocessing and Standardization in ML","Li, Z., et al. (2021). Understanding Robust Generalization in Deep Learning: A Median of Means Perspective. Proceedings of the 38th International Conference on Machine Learning (ICML), PMLR 139.","Theoretical paper exploring the connection between robust statistics (median) and model generalization, reinforcing that robust foundations lead to better performance."
"Bounded Activation and Smooth Saturating Functions","Elfwing, S., Uchibe, E., & Doya, K. (2018). Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning. Neural Networks, 107, 3-11.","Introduces activation functions combining linear and saturating components, highlighting the importance of smooth, bounded outputs for stable learning. Supports the use of tanh for a stable multiplier."
"Bounded Activation and Smooth Saturating Functions","Basirat, M., & Roth, P. M. (2021). The Use of Saturating Nonlinearities in Deep Learning: A Modern Perspective. Pattern Recognition Letters, 152, 315-321.","Modern review of tanh/sigmoid, discussing their renewed relevance where boundedness is a desired inductive bias (e.g., for output layers, or a bounded multiplier)."
"Modern Applications in IR, NLP, and Representation Learning","Rai, S., et al. (2022). A Modern Perspective on the Term Frequency in Document Scoring. ACM Transactions on Information Systems (TOIS).","Exemplifies the trend of re-evaluating classical IR heuristics (TF) with sophisticated, robust functions. Aligns with the research direction of improving IDF."
"Modern Applications in IR, NLP, and Representation Learning","Formal, T., et al. (2021). SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking. Proceedings of the 44th International Conference on Research and Development in Information Retrieval (SIGIR '21).","State-of-the-art model using a log-saturating function (log(1+ReLU(x))) on term weights. A conceptually similar operation to tanh for controlling weight distribution and improving robustness."
"Modern Applications in IR, NLP, and Representation Learning","Gao, L., et al. (2020). A Novel Adaptive Inverse Document Frequency for Document Representation. IEEE Access, 8, 136492-136503.","Closest direct comparison. Identifies the outlier problem in standard IDF and proposes a hard-capping solution (AIDF). The presented formula is a more general and powerful solution to the same problem."